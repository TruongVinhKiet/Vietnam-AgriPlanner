"""
Scraper: C√†o d·ªØ li·ªáu th·ª≠a ƒë·∫•t Huy·ªán Th·ªõi B√¨nh t·ª´ ilis.camau.gov.vn
======================================================================
S·ª≠ d·ª•ng WFS (Web Feature Service) t·ª´ GeoServer c·ªßa VNPT iLIS SDK
ƒë·ªÉ t·∫£i d·ªØ li·ªáu 129,000+ th·ª≠a ƒë·∫•t c·ªßa Huy·ªán Th·ªõi B√¨nh, C√† Mau.

T√≠nh nƒÉng:
- T·∫£i d·ªØ li·ªáu theo batch (1000 features/batch) ƒë·ªÉ tr√°nh timeout
- L∆∞u v√†o PostgreSQL (b·∫£ng land_parcels)
- T√≠nh t·ªça ƒë·ªô t√¢m t·ª´ geometry
- Map m√£ lo·∫°i ƒë·∫•t sang t√™n ti·∫øng Vi·ªát
- Resume t·ª´ batch cu·ªëi n·∫øu b·ªã gi√°n ƒëo·∫°n
- Hi·ªÉn th·ªã progress bar

S·ª≠ d·ª•ng:
    python scrape_land_parcels.py
"""

import json
import sys
import time
import math
import urllib.request
import urllib.error
import psycopg2
import psycopg2.extras
from datetime import datetime

# ============= C·∫§U H√åNH =============
WFS_BASE_URL = "https://ilis-sdk.vnpt.vn/map/geoserver/iLIS_CMU/wfs"
LAYER_NAME = "iLIS_CMU:cmu_thuadat_huyenthoibinh"
BATCH_SIZE = 1000  # S·ªë features m·ªói l·∫ßn request
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds

DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "dbname": "AgriPlanner",
    "user": "postgres",
    "password": "Kiet2004"
}

DISTRICT = "Th·ªõi B√¨nh"
PROVINCE = "C√† Mau"

# Th·ªùi B√¨nh bounding box (approximate)
THOI_BINH_BBOX = {
    "sw_lat": 9.18,
    "sw_lng": 104.95,
    "ne_lat": 9.48,
    "ne_lng": 105.32
}

# B·∫£ng m√£ lo·∫°i ƒë·∫•t ‚Üí t√™n ti·∫øng Vi·ªát (theo Lu·∫≠t ƒê·∫•t ƒëai)
LAND_USE_MAP = {
    "LUC": "ƒê·∫•t chuy√™n tr·ªìng l√∫a n∆∞·ªõc",
    "LUK": "ƒê·∫•t tr·ªìng l√∫a n∆∞·ªõc c√≤n l·∫°i",
    "LUN": "ƒê·∫•t l√∫a n∆∞∆°ng",
    "CHN": "ƒê·∫•t tr·ªìng c√¢y h√†ng nƒÉm kh√°c",
    "BHK": "ƒê·∫•t b·∫±ng tr·ªìng c√¢y h√†ng nƒÉm kh√°c",
    "HNK": "ƒê·∫•t n∆∞∆°ng r·∫´y",
    "CLN": "ƒê·∫•t tr·ªìng c√¢y l√¢u nƒÉm",
    "RSX": "ƒê·∫•t r·ª´ng s·∫£n xu·∫•t",
    "RPH": "ƒê·∫•t r·ª´ng ph√≤ng h·ªô",
    "RDD": "ƒê·∫•t r·ª´ng ƒë·∫∑c d·ª•ng",
    "NTS": "ƒê·∫•t nu√¥i tr·ªìng th·ªßy s·∫£n",
    "LMU": "ƒê·∫•t l√†m mu·ªëi",
    "NKH": "ƒê·∫•t n√¥ng nghi·ªáp kh√°c",
    "ONT": "ƒê·∫•t ·ªü n√¥ng th√¥n",
    "ODT": "ƒê·∫•t ·ªü ƒë√¥ th·ªã",
    "TSC": "ƒê·∫•t tr·ª• s·ªü c∆° quan",
    "DGD": "ƒê·∫•t c∆° s·ªü gi√°o d·ª•c ƒë√†o t·∫°o",
    "DYT": "ƒê·∫•t c∆° s·ªü y t·∫ø",
    "DVH": "ƒê·∫•t c∆° s·ªü vƒÉn h√≥a",
    "DTT": "ƒê·∫•t c∆° s·ªü th·ªÉ d·ª•c th·ªÉ thao",
    "DGT": "ƒê·∫•t giao th√¥ng",
    "DTL": "ƒê·∫•t th·ªßy l·ª£i",
    "DNL": "ƒê·∫•t c√¥ng tr√¨nh nƒÉng l∆∞·ª£ng",
    "DBV": "ƒê·∫•t c√¥ng tr√¨nh b∆∞u ch√≠nh vi·ªÖn th√¥ng",
    "SKC": "ƒê·∫•t c·ª•m khu c√¥ng nghi·ªáp",
    "SKK": "ƒê·∫•t khu kinh t·∫ø",
    "SKT": "ƒê·∫•t khu c√¥ng ngh·ªá cao",
    "TMD": "ƒê·∫•t th∆∞∆°ng m·∫°i d·ªãch v·ª•",
    "NTD": "ƒê·∫•t c∆° s·ªü nghƒ©a trang, nh√† tang l·ªÖ",
    "SON": "ƒê·∫•t m·∫∑t n∆∞·ªõc s√¥ng ng√≤i, k√™nh r·∫°ch, su·ªëi",
    "MNC": "ƒê·∫•t m·∫∑t n∆∞·ªõc chuy√™n d√πng",
    "PNK": "ƒê·∫•t phi n√¥ng nghi·ªáp kh√°c",
    "BCS": "ƒê·∫•t b·∫±ng ch∆∞a s·ª≠ d·ª•ng",
    "DCS": "ƒê·∫•t ƒë·ªìi ch∆∞a s·ª≠ d·ª•ng",
    "NCS": "N√∫i ƒë√° kh√¥ng c√≥ r·ª´ng c√¢y",
    "CSD": "ƒê·∫•t ch∆∞a s·ª≠ d·ª•ng",
    # M√£ k·∫øt h·ª£p ph·ªï bi·∫øn
    "ONT+CLN": "ƒê·∫•t ·ªü + C√¢y l√¢u nƒÉm",
    "CLN+LUK": "ƒê·∫•t c√¢y l√¢u nƒÉm + L√∫a",
    "NTS+CLN": "ƒê·∫•t th·ªßy s·∫£n + C√¢y l√¢u nƒÉm",
    "LUK+CLN": "ƒê·∫•t l√∫a + C√¢y l√¢u nƒÉm",
    "ONT+LUK": "ƒê·∫•t ·ªü + L√∫a",
    "CLN+NTS": "ƒê·∫•t c√¢y l√¢u nƒÉm + Th·ªßy s·∫£n",
    "LUK+NTS": "ƒê·∫•t l√∫a + Th·ªßy s·∫£n",
    "NTS+LUK": "ƒê·∫•t th·ªßy s·∫£n + L√∫a",
}


def lookup_land_use_name(code):
    """Tra c·ª©u t√™n lo·∫°i ƒë·∫•t t·ª´ m√£, h·ªó tr·ª£ m√£ k·∫øt h·ª£p"""
    if not code:
        return None
    code = code.strip()
    if code in LAND_USE_MAP:
        return LAND_USE_MAP[code]
    # Th·ª≠ t√°ch m√£ k·∫øt h·ª£p
    parts = code.replace("+", ",").replace("/", ",").split(",")
    names = []
    for p in parts:
        p = p.strip()
        if p in LAND_USE_MAP:
            names.append(LAND_USE_MAP[p])
        else:
            names.append(p)
    return " + ".join(names) if names else code


def calculate_centroid(geometry):
    """T√≠nh t·ªça ƒë·ªô t√¢m t·ª´ GeoJSON geometry"""
    if not geometry or not geometry.get("coordinates"):
        return None, None

    all_coords = []
    coords = geometry["coordinates"]
    geo_type = geometry.get("type", "")

    def extract_coords(c):
        if not c:
            return
        if isinstance(c[0], (int, float)):
            all_coords.append(c)
        else:
            for item in c:
                extract_coords(item)

    extract_coords(coords)

    if not all_coords:
        return None, None

    lngs = [c[0] for c in all_coords]
    lats = [c[1] for c in all_coords]
    return round(sum(lats) / len(lats), 7), round(sum(lngs) / len(lngs), 7)


def get_total_features():
    """L·∫•y t·ªïng s·ªë features t·ª´ WFS"""
    url = (
        f"{WFS_BASE_URL}?SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature"
        f"&typeName={LAYER_NAME}&resultType=hits"
    )
    for attempt in range(MAX_RETRIES):
        try:
            req = urllib.request.Request(url)
            r = urllib.request.urlopen(req, timeout=30)
            data = r.read().decode("utf-8")
            # Parse numberOfFeatures from XML
            import re
            match = re.search(r'numberOfFeatures="(\d+)"', data)
            if match:
                return int(match.group(1))
        except Exception as e:
            print(f"  Retry {attempt + 1}/{MAX_RETRIES}: {e}")
            time.sleep(RETRY_DELAY)
    return 0


def fetch_batch(start_index, count):
    """T·∫£i 1 batch features t·ª´ WFS s·ª≠ d·ª•ng CQL_FILTER pagination tr√™n objectid"""
    # NOTE: This GeoServer does NOT support WFS startIndex
    # We use CQL_FILTER objectid>last_id + sortBy=objectid for pagination
    raise NotImplementedError("Use fetch_by_objectid or fetch_by_bbox instead")


def fetch_by_objectid(last_objectid, count):
    """T·∫£i batch ti·∫øp theo d·ª±a tr√™n objectid > last_objectid"""
    import urllib.parse
    cql = f"objectid>{last_objectid}"
    url = (
        f"{WFS_BASE_URL}?SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature"
        f"&typeName={LAYER_NAME}"
        f"&outputFormat=application/json"
        f"&srsName=EPSG:4326"
        f"&maxFeatures={count}"
        f"&sortBy=objectid"
        f"&CQL_FILTER={urllib.parse.quote(cql)}"
    )
    for attempt in range(MAX_RETRIES):
        try:
            req = urllib.request.Request(url)
            req.add_header("Accept", "application/json")
            r = urllib.request.urlopen(req, timeout=120)
            raw = r.read()
            if not raw or raw[0:1] == b'<':
                # Got XML error instead of JSON
                time.sleep(RETRY_DELAY)
                continue
            data = json.loads(raw.decode("utf-8"))
            return data.get("features", [])
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))
    return []


def fetch_by_bbox(bbox, max_features=5000):
    """T·∫£i features trong 1 bounding box (cho records c√≥ objectid=0)"""
    import urllib.parse
    # bbox format: minx,miny,maxx,maxy (lng,lat)
    bbox_str = f"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}"
    cql = "objectid=0"
    url = (
        f"{WFS_BASE_URL}?SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature"
        f"&typeName={LAYER_NAME}"
        f"&outputFormat=application/json"
        f"&srsName=EPSG:4326"
        f"&maxFeatures={max_features}"
        f"&BBOX={bbox_str},EPSG:4326"
        f"&CQL_FILTER={urllib.parse.quote(cql)}"
    )
    for attempt in range(MAX_RETRIES):
        try:
            req = urllib.request.Request(url)
            req.add_header("Accept", "application/json")
            r = urllib.request.urlopen(req, timeout=120)
            raw = r.read()
            if not raw or raw[0:1] == b'<':
                time.sleep(RETRY_DELAY)
                continue
            data = json.loads(raw.decode("utf-8"))
            return data.get("features", [])
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))
    return []


def init_database(conn):
    """T·∫°o b·∫£ng n·∫øu ch∆∞a c√≥"""
    with open(
        "e:/Agriplanner/database/migrations/V41__land_parcels.sql", "r", encoding="utf-8"
    ) as f:
        sql = f.read()

    cur = conn.cursor()
    cur.execute(sql)
    conn.commit()
    cur.close()
    print("‚úì Database table ready")


def get_existing_count(conn):
    """ƒê·∫øm s·ªë records ƒë√£ c√≥ trong DB"""
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM land_parcels WHERE district = %s", (DISTRICT,))
    count = cur.fetchone()[0]
    cur.close()
    return count


def insert_parcels(conn, features):
    """Insert batch of features v√†o DB"""
    if not features:
        return 0

    insert_sql = """
        INSERT INTO land_parcels (
            object_id, parcel_id, map_sheet_id,
            map_sheet_number, parcel_number,
            area_sqm, legal_area_sqm,
            land_use_code, land_use_name,
            address, street_name, road, road_section, location,
            admin_unit_code, admin_unit_name, district, province,
            registration_status, change_status, spatial_status,
            area_zone, province_code,
            area_road, area_land, area_river, area_railway,
            boundary_geojson, center_lat, center_lng,
            notes
        ) VALUES (
            %(object_id)s, %(parcel_id)s, %(map_sheet_id)s,
            %(map_sheet_number)s, %(parcel_number)s,
            %(area_sqm)s, %(legal_area_sqm)s,
            %(land_use_code)s, %(land_use_name)s,
            %(address)s, %(street_name)s, %(road)s, %(road_section)s, %(location)s,
            %(admin_unit_code)s, %(admin_unit_name)s, %(district)s, %(province)s,
            %(registration_status)s, %(change_status)s, %(spatial_status)s,
            %(area_zone)s, %(province_code)s,
            %(area_road)s, %(area_land)s, %(area_river)s, %(area_railway)s,
            %(boundary_geojson)s, %(center_lat)s, %(center_lng)s,
            %(notes)s
        )
        ON CONFLICT (parcel_id) WHERE parcel_id IS NOT NULL DO NOTHING
    """

    rows = []
    for f in features:
        props = f.get("properties", {})
        geom = f.get("geometry")

        center_lat, center_lng = calculate_centroid(geom)
        land_code = props.get("loaidat")

        row = {
            "object_id": props.get("objectid"),
            "parcel_id": props.get("idthuadat"),
            "map_sheet_id": props.get("idtobando"),
            "map_sheet_number": props.get("tobandoso"),
            "parcel_number": props.get("sothututhua"),
            "area_sqm": props.get("dientich"),
            "legal_area_sqm": props.get("dientichpl"),
            "land_use_code": land_code,
            "land_use_name": lookup_land_use_name(land_code),
            "address": props.get("diachithua"),
            "street_name": props.get("tenduong"),
            "road": props.get("duong"),
            "road_section": props.get("doanduong"),
            "location": props.get("vitri"),
            "admin_unit_code": props.get("madvhc"),
            "admin_unit_name": props.get("tendvhc"),
            "district": DISTRICT,
            "province": PROVINCE,
            "registration_status": props.get("trangthaidangky"),
            "change_status": props.get("trangthaibiendong"),
            "spatial_status": props.get("trangthaikhonggian"),
            "area_zone": props.get("khuvuc"),
            "province_code": props.get("parmatinh"),
            "area_road": props.get("dientichhlgt"),
            "area_land": props.get("dientichhlld"),
            "area_river": props.get("dientichhlsongsuoi"),
            "area_railway": props.get("dientichhlduongsat"),
            "boundary_geojson": json.dumps(geom) if geom else None,
            "center_lat": center_lat,
            "center_lng": center_lng,
            "notes": props.get("ghichu"),
        }
        rows.append(row)

    cur = conn.cursor()
    inserted = 0
    for row in rows:
        try:
            cur.execute("SAVEPOINT sp1")
            cur.execute(insert_sql, row)
            if cur.rowcount > 0:
                inserted += 1
            cur.execute("RELEASE SAVEPOINT sp1")
        except Exception as e:
            cur.execute("ROLLBACK TO SAVEPOINT sp1")
            # Skip individual errors silently
            continue
    conn.commit()
    cur.close()
    return inserted


def print_progress(current, total, start_time, inserted_total):
    """Hi·ªÉn th·ªã progress bar ƒë·∫πp"""
    pct = current / total * 100 if total > 0 else 0
    bar_len = 40
    filled = int(bar_len * current / total) if total > 0 else 0
    bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)

    elapsed = time.time() - start_time
    if current > 0:
        eta = elapsed / current * (total - current)
        eta_str = f"{int(eta // 60)}m{int(eta % 60)}s"
    else:
        eta_str = "..."

    sys.stdout.write(
        f"\r  [{bar}] {pct:5.1f}% | {current:,}/{total:,} | "
        f"Inserted: {inserted_total:,} | ETA: {eta_str}  "
    )
    sys.stdout.flush()


def main():
    print("=" * 65)
    print("  üó∫Ô∏è  Scraper: Th·ª≠a ƒë·∫•t Huy·ªán Th·ªõi B√¨nh, C√† Mau")
    print("  üì° Ngu·ªìn: ilis.camau.gov.vn (WFS GeoServer)")
    print("=" * 65)
    print()

    # Step 1: Count total features
    print("üìä ƒêang ƒë·∫øm t·ªïng s·ªë th·ª≠a ƒë·∫•t...")
    total = get_total_features()
    if total == 0:
        print("‚ùå Kh√¥ng th·ªÉ l·∫•y t·ªïng s·ªë features. Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng.")
        return
    print(f"   T·ªïng c·ªông: {total:,} th·ª≠a ƒë·∫•t")
    print()

    # Step 2: Connect to DB
    print("üîó K·∫øt n·ªëi c∆° s·ªü d·ªØ li·ªáu...")
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        conn.set_client_encoding("UTF8")
        print(f"   ‚úì ƒê√£ k·∫øt n·ªëi PostgreSQL ({DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']})")
    except Exception as e:
        print(f"   ‚ùå L·ªói k·∫øt n·ªëi DB: {e}")
        return

    # Step 3: Init table
    print("üìã Kh·ªüi t·∫°o b·∫£ng land_parcels...")
    init_database(conn)

    # Step 4: Check existing data (for resume)
    existing = get_existing_count(conn)
    if existing > 0:
        print(f"   ‚ÑπÔ∏è  ƒê√£ c√≥ {existing:,} records trong DB")
        if existing >= total * 0.95:
            print("   ‚úì D·ªØ li·ªáu g·∫ßn ƒë·∫ßy ƒë·ªß.")

    # Step 5: Phase 1 - Scrape features with objectid > 0
    print()
    print(f"‚¨áÔ∏è  Phase 1: T·∫£i th·ª≠a ƒë·∫•t c√≥ objectid > 0...")

    # Find the last objectid we've already stored (for resume)
    cur = conn.cursor()
    cur.execute("SELECT COALESCE(MAX(object_id), 0) FROM land_parcels WHERE object_id > 0 AND district = %s", (DISTRICT,))
    resume_objectid = cur.fetchone()[0]
    cur.close()

    if resume_objectid > 0:
        print(f"   ‚ÑπÔ∏è  Resuming from objectid > {resume_objectid}")

    inserted_total = 0
    fetched_total = 0
    start_time = time.time()
    last_objectid = resume_objectid
    empty_batches = 0

    while True:
        features = fetch_by_objectid(last_objectid, BATCH_SIZE)
        if not features:
            empty_batches += 1
            if empty_batches >= 3:
                break
            time.sleep(2)
            continue

        empty_batches = 0
        fetched_total += len(features)
        inserted = insert_parcels(conn, features)
        inserted_total += inserted

        # Update last_objectid for next batch
        max_oid = max(f["properties"].get("objectid", 0) for f in features)
        last_objectid = max_oid

        print_progress(fetched_total, total, start_time, inserted_total)

        if len(features) < BATCH_SIZE:
            break  # Last batch

        time.sleep(0.3)  # Rate limiting

    print()
    print(f"   ‚úì Phase 1 ho√†n t·∫•t: {fetched_total:,} features, {inserted_total:,} inserted")

    # Step 6: Phase 2 - Scrape features with objectid = 0 using BBOX grid
    print()
    print(f"‚¨áÔ∏è  Phase 2: T·∫£i th·ª≠a ƒë·∫•t c√≥ objectid = 0 (d√πng BBOX grid)...")

    bbox = THOI_BINH_BBOX
    lat_range = bbox["ne_lat"] - bbox["sw_lat"]
    lng_range = bbox["ne_lng"] - bbox["sw_lng"]

    # Split into grid cells (6x6 = 36 cells)
    grid_rows = 6
    grid_cols = 6
    lat_step = lat_range / grid_rows
    lng_step = lng_range / grid_cols

    phase2_fetched = 0
    phase2_inserted = 0
    seen_uuids = set()  # Prevent cross-cell duplicates

    for row in range(grid_rows):
        for col in range(grid_cols):
            cell_sw_lng = bbox["sw_lng"] + col * lng_step
            cell_sw_lat = bbox["sw_lat"] + row * lat_step
            cell_ne_lng = cell_sw_lng + lng_step
            cell_ne_lat = cell_sw_lat + lat_step

            cell_bbox = (cell_sw_lng, cell_sw_lat, cell_ne_lng, cell_ne_lat)
            features = fetch_by_bbox(cell_bbox, max_features=5000)

            if features:
                # Deduplicate by parcel_id (idthuadat)
                unique_features = []
                for f in features:
                    uuid = f["properties"].get("idthuadat", "")
                    if uuid and uuid not in seen_uuids:
                        seen_uuids.add(uuid)
                        unique_features.append(f)

                if unique_features:
                    phase2_fetched += len(unique_features)
                    ins = insert_parcels(conn, unique_features)
                    phase2_inserted += ins

            cell_idx = row * grid_cols + col + 1
            total_cells = grid_rows * grid_cols
            sys.stdout.write(f"\r   Grid cell {cell_idx}/{total_cells} | Phase 2: {phase2_fetched:,} fetched, {phase2_inserted:,} inserted  ")
            sys.stdout.flush()
            time.sleep(0.3)

    print()
    print(f"   ‚úì Phase 2 ho√†n t·∫•t: {phase2_fetched:,} features, {phase2_inserted:,} inserted")

    # Final summary
    elapsed = time.time() - start_time
    total_fetched = fetched_total + phase2_fetched
    total_inserted = inserted_total + phase2_inserted
    print()
    print("=" * 65)
    print(f"  ‚úÖ HO√ÄN T·∫§T!")
    print(f"  üì• ƒê√£ t·∫£i:   {total_fetched:,} th·ª≠a ƒë·∫•t")
    print(f"  üíæ ƒê√£ l∆∞u:   {total_inserted:,} records m·ªõi")
    print(f"  ‚è±Ô∏è  Th·ªùi gian: {int(elapsed // 60)} ph√∫t {int(elapsed % 60)} gi√¢y")
    print("=" * 65)

    # Step 6: Summary statistics
    print()
    print("üìä Th·ªëng k√™ theo lo·∫°i ƒë·∫•t:")
    cur = conn.cursor()
    cur.execute("""
        SELECT land_use_code, land_use_name, COUNT(*) as cnt,
               ROUND(SUM(area_sqm)::numeric, 0) as total_area
        FROM land_parcels
        WHERE district = %s
        GROUP BY land_use_code, land_use_name
        ORDER BY cnt DESC
        LIMIT 15
    """, (DISTRICT,))
    rows = cur.fetchall()
    print(f"   {'M√£':<12} {'T√™n':<35} {'S·ªë th·ª≠a':>8} {'DT (m¬≤)':>15}")
    print(f"   {'-'*12} {'-'*35} {'-'*8} {'-'*15}")
    for code, name, cnt, area in rows:
        name_display = (name or code or "?")[:35]
        area_str = f"{area:,.0f}" if area else "N/A"
        print(f"   {(code or '?'):<12} {name_display:<35} {cnt:>8,} {area_str:>15}")
    cur.close()

    print()
    print("üìä Th·ªëng k√™ theo x√£:")
    cur = conn.cursor()
    cur.execute("""
        SELECT admin_unit_name, COUNT(*) as cnt,
               ROUND(SUM(area_sqm)::numeric / 10000, 2) as total_ha
        FROM land_parcels
        WHERE district = %s
        GROUP BY admin_unit_name
        ORDER BY cnt DESC
    """, (DISTRICT,))
    rows = cur.fetchall()
    print(f"   {'X√£/Ph∆∞·ªùng':<30} {'S·ªë th·ª≠a':>8} {'DT (ha)':>12}")
    print(f"   {'-'*30} {'-'*8} {'-'*12}")
    for name, cnt, ha in rows:
        ha_str = f"{ha:,.2f}" if ha else "N/A"
        print(f"   {(name or '?'):<30} {cnt:>8,} {ha_str:>12}")
    cur.close()

    conn.close()


if __name__ == "__main__":
    main()
