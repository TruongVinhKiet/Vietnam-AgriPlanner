"""
Scraper: C√†o d·ªØ li·ªáu th·ª≠a ƒë·∫•t TO√ÄN B·ªò C√† Mau t·ª´ ilis.camau.gov.vn
=====================================================================
M·ªü r·ªông t·ª´ scrape_land_parcels.py ƒë·ªÉ c√†o t·∫•t c·∫£ 9 huy·ªán/th√†nh ph·ªë.
S·ª≠ d·ª•ng WFS (Web Feature Service) t·ª´ GeoServer c·ªßa VNPT iLIS SDK.

Huy·ªán/Th√†nh ph·ªë C√† Mau (c√≥ tr√™n GeoServer):
1. Th·ªõi B√¨nh (ƒë√£ c√†o ~129,000 th·ª≠a)
2. C√°i N∆∞·ªõc
3. ƒê·∫ßm D∆°i
4. Ph√∫ T√¢n
5. Tr·∫ßn VƒÉn Th·ªùi
6. U Minh
7. TP C√† Mau

L∆∞u √Ω: NƒÉm CƒÉn v√† Ng·ªçc Hi·ªÉn ch∆∞a c√≥ layer tr√™n GeoServer iLIS.

S·ª≠ d·ª•ng:
    python scrape_all_districts.py                 # C√†o t·∫•t c·∫£ huy·ªán ch∆∞a c√†o
    python scrape_all_districts.py --district "C√°i N∆∞·ªõc"  # C√†o 1 huy·ªán c·ª• th·ªÉ
    python scrape_all_districts.py --list           # Li·ªát k√™ layer names
    python scrape_all_districts.py --check          # Ki·ªÉm tra tr·∫°ng th√°i
    python scrape_all_districts.py --skip-done      # B·ªè qua huy·ªán ƒë√£ c√†o
"""

import json
import sys
import time
import math
import argparse
import urllib.request
import urllib.error
import psycopg2
import psycopg2.extras
from datetime import datetime

# ============= C·∫§U H√åNH =============
WFS_BASE_URL = "https://ilis-sdk.vnpt.vn/map/geoserver/iLIS_CMU/wfs"
BATCH_SIZE = 1000
MAX_RETRIES = 3
RETRY_DELAY = 5

DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "dbname": "AgriPlanner",
    "user": "postgres",
    "password": "Kiet2004"
}

PROVINCE = "C√† Mau"

# ============= DANH S√ÅCH HUY·ªÜN =============
# Layer names theo pattern: iLIS_CMU:cmu_thuadat_huyen<t√™n_kh√¥ng_d·∫•u_vi·∫øt_li·ªÅn>
# ho·∫∑c iLIS_CMU:cmu_thuadat_tp<t√™n>
DISTRICTS = {
    "Th·ªõi B√¨nh": {
        "layer": "iLIS_CMU:cmu_thuadat_huyenthoibinh",
        "bbox": {"sw_lat": 9.18, "sw_lng": 104.95, "ne_lat": 9.48, "ne_lng": 105.32}
    },
    "C√°i N∆∞·ªõc": {
        "layer": "iLIS_CMU:cmu_thuadat_huyencainuoc",
        "bbox": {"sw_lat": 8.85, "sw_lng": 104.85, "ne_lat": 9.15, "ne_lng": 105.15}
    },
    "ƒê·∫ßm D∆°i": {
        "layer": "iLIS_CMU:cmu_thuadat_huyendamdoi",
        "bbox": {"sw_lat": 8.80, "sw_lng": 105.00, "ne_lat": 9.10, "ne_lng": 105.35}
    },
    "Ph√∫ T√¢n": {
        "layer": "iLIS_CMU:cmu_thuadat_huyenphutan",
        "bbox": {"sw_lat": 8.90, "sw_lng": 104.85, "ne_lat": 9.15, "ne_lng": 105.10}
    },
    "Tr·∫ßn VƒÉn Th·ªùi": {
        "layer": "iLIS_CMU:cmu_thuadat_huyentranvanthoi",
        "bbox": {"sw_lat": 9.00, "sw_lng": 104.80, "ne_lat": 9.35, "ne_lng": 105.10}
    },
    "U Minh": {
        "layer": "iLIS_CMU:cmu_thuadat_huyenuminh",
        "bbox": {"sw_lat": 9.20, "sw_lng": 104.85, "ne_lat": 9.55, "ne_lng": 105.10}
    },
    "TP C√† Mau": {
        "layer": "iLIS_CMU:cmu_thuadat_tpcamau",
        "bbox": {"sw_lat": 9.10, "sw_lng": 105.10, "ne_lat": 9.25, "ne_lng": 105.25}
    },
}

# B·∫£ng m√£ lo·∫°i ƒë·∫•t ‚Üí t√™n ti·∫øng Vi·ªát
LAND_USE_MAP = {
    "LUC": "ƒê·∫•t chuy√™n tr·ªìng l√∫a n∆∞·ªõc", "LUK": "ƒê·∫•t tr·ªìng l√∫a n∆∞·ªõc c√≤n l·∫°i",
    "LUN": "ƒê·∫•t l√∫a n∆∞∆°ng", "CHN": "ƒê·∫•t tr·ªìng c√¢y h√†ng nƒÉm kh√°c",
    "BHK": "ƒê·∫•t b·∫±ng tr·ªìng c√¢y h√†ng nƒÉm kh√°c", "HNK": "ƒê·∫•t n∆∞∆°ng r·∫´y",
    "CLN": "ƒê·∫•t tr·ªìng c√¢y l√¢u nƒÉm", "RSX": "ƒê·∫•t r·ª´ng s·∫£n xu·∫•t",
    "RPH": "ƒê·∫•t r·ª´ng ph√≤ng h·ªô", "RDD": "ƒê·∫•t r·ª´ng ƒë·∫∑c d·ª•ng",
    "NTS": "ƒê·∫•t nu√¥i tr·ªìng th·ªßy s·∫£n", "LMU": "ƒê·∫•t l√†m mu·ªëi",
    "NKH": "ƒê·∫•t n√¥ng nghi·ªáp kh√°c", "ONT": "ƒê·∫•t ·ªü n√¥ng th√¥n",
    "ODT": "ƒê·∫•t ·ªü ƒë√¥ th·ªã", "TSC": "ƒê·∫•t tr·ª• s·ªü c∆° quan",
    "DGD": "ƒê·∫•t c∆° s·ªü gi√°o d·ª•c ƒë√†o t·∫°o", "DYT": "ƒê·∫•t c∆° s·ªü y t·∫ø",
    "DVH": "ƒê·∫•t c∆° s·ªü vƒÉn h√≥a", "DTT": "ƒê·∫•t c∆° s·ªü th·ªÉ d·ª•c th·ªÉ thao",
    "DGT": "ƒê·∫•t giao th√¥ng", "DTL": "ƒê·∫•t th·ªßy l·ª£i",
    "DNL": "ƒê·∫•t c√¥ng tr√¨nh nƒÉng l∆∞·ª£ng", "DBV": "ƒê·∫•t c√¥ng tr√¨nh b∆∞u ch√≠nh vi·ªÖn th√¥ng",
    "SKC": "ƒê·∫•t c·ª•m khu c√¥ng nghi·ªáp", "SKK": "ƒê·∫•t khu kinh t·∫ø",
    "SKT": "ƒê·∫•t khu c√¥ng ngh·ªá cao", "TMD": "ƒê·∫•t th∆∞∆°ng m·∫°i d·ªãch v·ª•",
    "NTD": "ƒê·∫•t c∆° s·ªü nghƒ©a trang, nh√† tang l·ªÖ",
    "SON": "ƒê·∫•t m·∫∑t n∆∞·ªõc s√¥ng ng√≤i, k√™nh r·∫°ch, su·ªëi",
    "MNC": "ƒê·∫•t m·∫∑t n∆∞·ªõc chuy√™n d√πng", "PNK": "ƒê·∫•t phi n√¥ng nghi·ªáp kh√°c",
    "BCS": "ƒê·∫•t b·∫±ng ch∆∞a s·ª≠ d·ª•ng", "DCS": "ƒê·∫•t ƒë·ªìi ch∆∞a s·ª≠ d·ª•ng",
    "NCS": "N√∫i ƒë√° kh√¥ng c√≥ r·ª´ng c√¢y", "CSD": "ƒê·∫•t ch∆∞a s·ª≠ d·ª•ng",
    "TIN": "ƒê·∫•t t√¥n gi√°o", "CQP": "ƒê·∫•t qu·ªëc ph√≤ng", "CAN": "ƒê·∫•t an ninh",
    "DHT": "ƒê·∫•t c√¥ng tr√¨nh h·∫° t·∫ßng", "TDP": "ƒê·∫•t t√¥n gi√°o, t√≠n ng∆∞·ª°ng",
    "ONT+CLN": "ƒê·∫•t ·ªü + C√¢y l√¢u nƒÉm", "CLN+LUK": "ƒê·∫•t c√¢y l√¢u nƒÉm + L√∫a",
    "NTS+CLN": "ƒê·∫•t th·ªßy s·∫£n + C√¢y l√¢u nƒÉm", "LUK+CLN": "ƒê·∫•t l√∫a + C√¢y l√¢u nƒÉm",
    "ONT+LUK": "ƒê·∫•t ·ªü + L√∫a", "CLN+NTS": "ƒê·∫•t c√¢y l√¢u nƒÉm + Th·ªßy s·∫£n",
    "LUK+NTS": "ƒê·∫•t l√∫a + Th·ªßy s·∫£n", "NTS+LUK": "ƒê·∫•t th·ªßy s·∫£n + L√∫a",
}


def lookup_land_use_name(code):
    if not code:
        return None
    code = code.strip()
    if code in LAND_USE_MAP:
        return LAND_USE_MAP[code]
    parts = code.replace("+", ",").replace("/", ",").split(",")
    names = []
    for p in parts:
        p = p.strip()
        names.append(LAND_USE_MAP.get(p, p))
    return " + ".join(names) if names else None


def get_db_connection():
    return psycopg2.connect(**DB_CONFIG)


def ensure_table_exists(conn):
    """Ensure the land_parcels table exists"""
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS land_parcels (
            id SERIAL PRIMARY KEY,
            parcel_id VARCHAR(100),
            object_id BIGINT,
            map_sheet_number VARCHAR(50),
            parcel_number VARCHAR(50),
            area_sqm DOUBLE PRECISION,
            legal_area_sqm DOUBLE PRECISION,
            land_use_code VARCHAR(30),
            land_use_name VARCHAR(100),
            address TEXT,
            admin_unit_code VARCHAR(20),
            admin_unit_name VARCHAR(100),
            district VARCHAR(100),
            province VARCHAR(50) DEFAULT 'C√† Mau',
            center_lat DOUBLE PRECISION,
            center_lng DOUBLE PRECISION,
            boundary_geojson JSONB,
            owner_name VARCHAR(200),
            owner_address TEXT,
            certificate_number VARCHAR(100),
            certificate_date DATE,
            source_system VARCHAR(50) DEFAULT 'iLIS',
            raw_properties JSONB,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(parcel_id)
        );
    """)
    # Add indexes
    for idx_sql in [
        "CREATE INDEX IF NOT EXISTS idx_land_parcels_district ON land_parcels(district);",
        "CREATE INDEX IF NOT EXISTS idx_land_parcels_admin_unit ON land_parcels(admin_unit_name);",
        "CREATE INDEX IF NOT EXISTS idx_land_parcels_land_use ON land_parcels(land_use_code);",
        "CREATE INDEX IF NOT EXISTS idx_land_parcels_location ON land_parcels(center_lat, center_lng);",
        "CREATE INDEX IF NOT EXISTS idx_land_parcels_geom ON land_parcels USING GIN(boundary_geojson);",
    ]:
        try:
            cur.execute(idx_sql)
        except Exception:
            pass
    conn.commit()
    cur.close()

    # Add columns that may be missing from older schema
    alter_columns = [
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS raw_properties JSONB;",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS owner_name VARCHAR(200);",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS owner_address TEXT;",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS certificate_number VARCHAR(100);",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS certificate_date DATE;",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS source_system VARCHAR(50) DEFAULT 'iLIS';",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS scrape_batch VARCHAR(50);",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS data_version INTEGER DEFAULT 1;",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP;",
        "ALTER TABLE land_parcels ADD COLUMN IF NOT EXISTS created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP;",
        # Fix column types for data compatibility
        "ALTER TABLE land_parcels ALTER COLUMN object_id TYPE BIGINT;",
        "ALTER TABLE land_parcels ALTER COLUMN parcel_number TYPE VARCHAR(50);",
        # Ensure unique constraint on parcel_id for ON CONFLICT
        "CREATE UNIQUE INDEX IF NOT EXISTS idx_land_parcels_parcel_id ON land_parcels (parcel_id);",
    ]
    for col_sql in alter_columns:
        try:
            cur2 = conn.cursor()
            cur2.execute(col_sql)
            conn.commit()
            cur2.close()
        except Exception:
            conn.rollback()


def compute_centroid(geometry):
    """Calculate approximate centroid from GeoJSON geometry"""
    coords = geometry.get("coordinates", [])
    geo_type = geometry.get("type", "")

    all_points = []

    def flatten_coords(c, depth=0):
        if depth > 5:
            return
        if isinstance(c, list) and len(c) >= 2 and isinstance(c[0], (int, float)):
            all_points.append(c)
        elif isinstance(c, list):
            for item in c:
                flatten_coords(item, depth + 1)

    flatten_coords(coords)

    if not all_points:
        return None, None

    avg_lng = sum(p[0] for p in all_points) / len(all_points)
    avg_lat = sum(p[1] for p in all_points) / len(all_points)
    return avg_lat, avg_lng


def fetch_wfs_features(layer_name, cql_filter=None, bbox=None, max_features=BATCH_SIZE):
    """Fetch features from WFS service"""
    params = {
        "service": "WFS",
        "version": "2.0.0",
        "request": "GetFeature",
        "typeName": layer_name,
        "outputFormat": "application/json",
        "count": str(max_features),
        "srsName": "EPSG:4326",
    }

    if cql_filter:
        params["CQL_FILTER"] = cql_filter
    if bbox:
        params["BBOX"] = bbox

    query_string = "&".join(f"{k}={urllib.request.quote(str(v))}" for k, v in params.items())
    url = f"{WFS_BASE_URL}?{query_string}"

    for attempt in range(MAX_RETRIES):
        try:
            req = urllib.request.Request(url, headers={
                "User-Agent": "AgriPlanner-Scraper/2.0",
                "Accept": "application/json"
            })
            with urllib.request.urlopen(req, timeout=120) as resp:
                data = json.loads(resp.read().decode("utf-8"))
                return data.get("features", [])
        except (urllib.error.URLError, urllib.error.HTTPError, json.JSONDecodeError) as e:
            if attempt < MAX_RETRIES - 1:
                print(f"\n   ‚ö† Retry {attempt + 1}/{MAX_RETRIES}: {str(e)[:80]}")
                time.sleep(RETRY_DELAY * (attempt + 1))
            else:
                print(f"\n   ‚ùå Failed after {MAX_RETRIES} retries: {str(e)[:100]}")
                return []
        except Exception as e:
            print(f"\n   ‚ùå Unexpected error: {e}")
            return []


def insert_parcels(conn, features, district_name):
    """Insert features into PostgreSQL"""
    if not features:
        return 0

    inserted = 0
    cur = conn.cursor()

    for f in features:
        props = f.get("properties", {})
        geometry = f.get("geometry")
        
        parcel_id = props.get("uuid") or props.get("objectid") or f.get("id", "")
        if not parcel_id:
            continue

        parcel_id = str(parcel_id)
        center_lat, center_lng = compute_centroid(geometry) if geometry else (None, None)
        land_use_code = (props.get("loaidat") or "").strip()
        land_use_name = lookup_land_use_name(land_use_code)

        try:
            cur.execute("SAVEPOINT sp_insert")
            cur.execute("""
                INSERT INTO land_parcels (
                    parcel_id, object_id, map_sheet_number, parcel_number,
                    area_sqm, legal_area_sqm, land_use_code, land_use_name,
                    address, admin_unit_code, admin_unit_name,
                    district, province, center_lat, center_lng,
                    boundary_geojson, raw_properties, source_system
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, 'iLIS'
                )
                ON CONFLICT (parcel_id) DO UPDATE SET
                    area_sqm = EXCLUDED.area_sqm,
                    land_use_code = EXCLUDED.land_use_code,
                    land_use_name = EXCLUDED.land_use_name,
                    boundary_geojson = EXCLUDED.boundary_geojson,
                    updated_at = CURRENT_TIMESTAMP
            """, (
                parcel_id,
                props.get("objectid"),
                props.get("tobandoso"),
                props.get("sothututhua"),
                props.get("dientich"),
                props.get("dientichpl"),
                land_use_code or None,
                land_use_name,
                props.get("diachithua"),
                props.get("madvhc"),
                props.get("tendvhc"),
                district_name,
                PROVINCE,
                center_lat,
                center_lng,
                json.dumps(geometry) if geometry else None,
                json.dumps(props),
            ))
            cur.execute("RELEASE SAVEPOINT sp_insert")
            inserted += 1
        except psycopg2.errors.UniqueViolation:
            cur.execute("ROLLBACK TO SAVEPOINT sp_insert")
        except Exception as e:
            cur.execute("ROLLBACK TO SAVEPOINT sp_insert")
            print(f"\n   ‚ö† Insert error: {str(e)[:80]}")

    conn.commit()
    cur.close()
    return inserted


def check_layer_exists(layer_name):
    """Check if a WFS layer exists by requesting 1 feature"""
    try:
        features = fetch_wfs_features(layer_name, max_features=1)
        return len(features) > 0
    except Exception:
        return False


def get_district_count(conn, district_name):
    """Get number of parcels already in DB for a district"""
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM land_parcels WHERE district = %s", (district_name,))
    count = cur.fetchone()[0]
    cur.close()
    return count


def scrape_district(conn, district_name, district_config, skip_if_exists=False):
    """Scrape all land parcels for a single district"""
    layer_name = district_config["layer"]
    bbox = district_config["bbox"]
    
    existing_count = get_district_count(conn, district_name)
    
    print(f"\n{'='*65}")
    print(f"  üèòÔ∏è  Huy·ªán/TP: {district_name}")
    print(f"  üì° Layer:    {layer_name}")
    print(f"  üíæ ƒê√£ c√≥:    {existing_count:,} th·ª≠a trong DB")
    print(f"{'='*65}")

    if skip_if_exists and existing_count > 1000:
        print(f"  ‚è© B·ªè qua (ƒë√£ c√≥ {existing_count:,} th·ª≠a)")
        return existing_count, 0

    # Check layer exists
    print(f"  üîç Ki·ªÉm tra layer...")
    if not check_layer_exists(layer_name):
        print(f"  ‚ùå Layer kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng tr·∫£ d·ªØ li·ªáu!")
        print(f"     Th·ª≠ c√°c t√™n layer kh√°c...")
        
        # Try alternate layer name patterns
        alt_names = [
            layer_name.replace("huyen", "h."),
            layer_name.replace("cmu_thuadat_", "cmu_thuadat_h"),
            f"iLIS_CMU:cmu_thuadat_{district_name.lower().replace(' ', '')}",
        ]
        found = False
        for alt in alt_names:
            if check_layer_exists(alt):
                print(f"  ‚úì T√¨m th·∫•y layer: {alt}")
                layer_name = alt
                found = True
                break
        
        if not found:
            print(f"  ‚ö† Kh√¥ng t√¨m th·∫•y layer cho {district_name}. B·ªè qua.")
            return existing_count, 0

    # Phase 1: Paginate by objectid > last_id
    print(f"\n  üì• Phase 1: Pagination by objectid...")
    start_time = time.time()
    fetched_total = 0
    inserted_total = 0
    last_id = 0
    empty_batches = 0

    while True:
        cql_filter = f"objectid>{last_id}"
        features = fetch_wfs_features(layer_name, cql_filter=cql_filter)

        if not features:
            empty_batches += 1
            if empty_batches >= 2:
                break
            last_id += BATCH_SIZE
            continue

        empty_batches = 0
        fetched_total += len(features)

        # Find max objectid for next batch
        max_oid = 0
        for f in features:
            oid = f.get("properties", {}).get("objectid", 0) or 0
            try:
                oid = int(oid)
            except (ValueError, TypeError):
                oid = 0
            if oid > max_oid:
                max_oid = oid

        if max_oid <= last_id:
            break
        last_id = max_oid

        ins = insert_parcels(conn, features, district_name)
        inserted_total += ins

        sys.stdout.write(f"\r   Batch: objectid>{last_id:,} | T·∫£i: {fetched_total:,} | L∆∞u: {inserted_total:,}  ")
        sys.stdout.flush()
        time.sleep(0.3)

    print(f"\n   ‚úì Phase 1: {fetched_total:,} features, {inserted_total:,} inserted")

    # Phase 2: Grid-based scraping for records missed by objectid pagination
    print(f"\n  üì• Phase 2: Grid-based (BBOX only)...")
    grid_rows, grid_cols = 5, 5
    lat_step = (bbox["ne_lat"] - bbox["sw_lat"]) / grid_rows
    lng_step = (bbox["ne_lng"] - bbox["sw_lng"]) / grid_cols
    phase2_fetched = 0
    phase2_inserted = 0
    seen_uuids = set()

    for row in range(grid_rows):
        for col in range(grid_cols):
            cell_sw_lat = bbox["sw_lat"] + row * lat_step
            cell_sw_lng = bbox["sw_lng"] + col * lng_step
            cell_ne_lat = cell_sw_lat + lat_step
            cell_ne_lng = cell_sw_lng + lng_step

            bbox_str = f"{cell_sw_lng},{cell_sw_lat},{cell_ne_lng},{cell_ne_lat}"
            try:
                features = fetch_wfs_features(
                    layer_name,
                    bbox=bbox_str
                )
            except Exception:
                features = []

            if features:
                unique = []
                for f in features:
                    uid = f.get("properties", {}).get("uuid", "")
                    if uid and uid not in seen_uuids:
                        seen_uuids.add(uid)
                        unique.append(f)

                if unique:
                    phase2_fetched += len(unique)
                    ins = insert_parcels(conn, unique, district_name)
                    phase2_inserted += ins

            cell_idx = row * grid_cols + col + 1
            sys.stdout.write(f"\r   Grid {cell_idx}/{grid_rows*grid_cols} | Phase 2: {phase2_fetched:,} fetched, {phase2_inserted:,} inserted  ")
            sys.stdout.flush()
            time.sleep(0.3)

    print(f"\n   ‚úì Phase 2: {phase2_fetched:,} features, {phase2_inserted:,} inserted")

    elapsed = time.time() - start_time
    total_new = inserted_total + phase2_inserted
    total_fetched = fetched_total + phase2_fetched
    
    print(f"\n  ‚úÖ {district_name}: {total_fetched:,} fetched, {total_new:,} new | ‚è± {int(elapsed//60)}m{int(elapsed%60)}s")
    
    return get_district_count(conn, district_name), total_new


def print_summary(conn):
    """Print summary statistics for all districts"""
    print(f"\n{'='*65}")
    print(f"  üìä T·ªîNG K·∫æT TO√ÄN T·ªàNH C√Ä MAU")
    print(f"{'='*65}")

    cur = conn.cursor()
    
    # By district
    cur.execute("""
        SELECT district, COUNT(*) as cnt,
               ROUND(SUM(area_sqm)::numeric / 10000, 2) as total_ha
        FROM land_parcels
        WHERE province = %s
        GROUP BY district
        ORDER BY cnt DESC
    """, (PROVINCE,))
    rows = cur.fetchall()
    
    print(f"\n  {'Huy·ªán/TP':<25} {'S·ªë th·ª≠a':>10} {'DT (ha)':>12}")
    print(f"  {'-'*25} {'-'*10} {'-'*12}")
    grand_total = 0
    grand_ha = 0
    for name, cnt, ha in rows:
        ha_str = f"{ha:,.2f}" if ha else "N/A"
        print(f"  {(name or '?'):<25} {cnt:>10,} {ha_str:>12}")
        grand_total += cnt
        grand_ha += (ha or 0)
    print(f"  {'-'*25} {'-'*10} {'-'*12}")
    print(f"  {'T·ªîNG':<25} {grand_total:>10,} {grand_ha:>12,.2f}")

    # Top land use types
    print(f"\n  üìä Top 10 lo·∫°i ƒë·∫•t:")
    cur.execute("""
        SELECT land_use_code, land_use_name, COUNT(*) as cnt,
               ROUND(SUM(area_sqm)::numeric / 10000, 2) as total_ha
        FROM land_parcels
        WHERE province = %s
        GROUP BY land_use_code, land_use_name
        ORDER BY cnt DESC
        LIMIT 10
    """, (PROVINCE,))
    rows = cur.fetchall()
    print(f"  {'M√£':<10} {'T√™n':<35} {'S·ªë th·ª≠a':>8} {'DT (ha)':>12}")
    print(f"  {'-'*10} {'-'*35} {'-'*8} {'-'*12}")
    for code, name, cnt, ha in rows:
        name_d = (name or code or "?")[:35]
        ha_str = f"{ha:,.2f}" if ha else "N/A"
        print(f"  {(code or '?'):<10} {name_d:<35} {cnt:>8,} {ha_str:>12}")

    cur.close()


def main():
    parser = argparse.ArgumentParser(description="C√†o d·ªØ li·ªáu th·ª≠a ƒë·∫•t to√†n t·ªânh C√† Mau")
    parser.add_argument("--district", type=str, help="C√†o 1 huy·ªán c·ª• th·ªÉ (t√™n ti·∫øng Vi·ªát)")
    parser.add_argument("--list", action="store_true", help="Li·ªát k√™ layer names")
    parser.add_argument("--check", action="store_true", help="Ki·ªÉm tra layer c√≥ t·ªìn t·∫°i")
    parser.add_argument("--skip-done", action="store_true", help="B·ªè qua huy·ªán ƒë√£ c√†o (>1000 th·ª≠a)")
    parser.add_argument("--summary", action="store_true", help="Ch·ªâ in th·ªëng k√™")
    args = parser.parse_args()

    if args.list:
        print("\nüìã Danh s√°ch layer names:")
        for name, cfg in DISTRICTS.items():
            print(f"   {name:<20} ‚Üí {cfg['layer']}")
        return

    conn = get_db_connection()
    ensure_table_exists(conn)

    if args.check:
        print("\nüîç Ki·ªÉm tra layer t·ªìn t·∫°i:")
        for name, cfg in DISTRICTS.items():
            exists = check_layer_exists(cfg["layer"])
            count = get_district_count(conn, name)
            status = "‚úÖ" if exists else "‚ùå"
            print(f"   {status} {name:<20} Layer: {cfg['layer']:<50} DB: {count:,}")
        conn.close()
        return

    if args.summary:
        print_summary(conn)
        conn.close()
        return

    print()
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë  üó∫Ô∏è  SCRAPER D·ªÆ LI·ªÜU TH·ª¨ ƒê·∫§T TO√ÄN T·ªàNH C√Ä MAU         ‚ïë")
    print("‚ïë  Ngu·ªìn: ilis.camau.gov.vn (GeoServer WFS)                ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")

    districts_to_scrape = dict(DISTRICTS)
    if args.district:
        # Find matching district
        matched = None
        for name in DISTRICTS:
            if name.lower() == args.district.lower() or args.district.lower() in name.lower():
                matched = name
                break
        if not matched:
            print(f"\n‚ùå Kh√¥ng t√¨m th·∫•y huy·ªán '{args.district}'")
            print(f"   C√°c huy·ªán: {', '.join(DISTRICTS.keys())}")
            conn.close()
            return
        districts_to_scrape = {matched: DISTRICTS[matched]}
    else:
        # M·∫∑c ƒë·ªãnh b·ªè qua Th·ªõi B√¨nh (ƒë√£ c√≥ ~129K th·ª≠a trong DB)
        tb_count = get_district_count(conn, "Th·ªõi B√¨nh")
        if tb_count > 1000:
            print(f"\n  ‚è© T·ª± ƒë·ªông b·ªè qua Th·ªõi B√¨nh (ƒë√£ c√≥ {tb_count:,} th·ª≠a trong DB)")
            districts_to_scrape.pop("Th·ªõi B√¨nh", None)

    # Scrape each district
    start_all = time.time()
    results = {}

    for idx, (name, cfg) in enumerate(districts_to_scrape.items(), 1):
        print(f"\n{'‚ñì'*65}")
        print(f"  [{idx}/{len(districts_to_scrape)}] ƒêang x·ª≠ l√Ω: {name}")
        print(f"{'‚ñì'*65}")

        try:
            total, new = scrape_district(conn, name, cfg, skip_if_exists=args.skip_done)
            results[name] = {"total": total, "new": new, "status": "‚úÖ"}
        except KeyboardInterrupt:
            print(f"\n\n‚ö† D·ª´ng b·ªüi ng∆∞·ªùi d√πng. ƒê√£ ho√†n t·∫•t ƒë·∫øn huy·ªán {name}.")
            results[name] = {"total": get_district_count(conn, name), "new": 0, "status": "‚ö†Ô∏è Interrupted"}
            break
        except Exception as e:
            print(f"\n‚ùå L·ªói khi c√†o {name}: {e}")
            results[name] = {"total": get_district_count(conn, name), "new": 0, "status": f"‚ùå {str(e)[:50]}"}

    # Final summary
    elapsed_all = time.time() - start_all
    print(f"\n\n{'='*65}")
    print(f"  üìä K·∫æT QU·∫¢ CH·∫†Y")
    print(f"{'='*65}")
    print(f"  {'Huy·ªán/TP':<20} {'Status':<12} {'T·ªïng DB':>10} {'M·ªõi':>8}")
    print(f"  {'-'*20} {'-'*12} {'-'*10} {'-'*8}")
    for name, r in results.items():
        print(f"  {name:<20} {r['status']:<12} {r['total']:>10,} {r['new']:>8,}")
    print(f"\n  ‚è± T·ªïng th·ªùi gian: {int(elapsed_all//60)}m{int(elapsed_all%60)}s")

    # Print overall summary
    print_summary(conn)
    conn.close()


if __name__ == "__main__":
    main()
